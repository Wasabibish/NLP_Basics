{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook we re going to introduce text processing in NLP (Natural Language Processing), this will include :\n",
    "* Tokenization\n",
    "* Stop words removal\n",
    "* Stemming and Lemmatization\n",
    "* Part-of-Speech Tagging\n",
    "\n",
    "We will be using different librairies\n",
    "\n",
    "NLTK, TextBlob, spacy, Gensim, these are the main known libraries for basic NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units called tokens, which could be words, phrases, or characters. It's the first step in most NLP tasks, allowing further analysis on individual components of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import (\n",
    "    word_tokenize,\n",
    "    sent_tokenize,\n",
    "    TreebankWordTokenizer,\n",
    "    wordpunct_tokenize,\n",
    "    TweetTokenizer,\n",
    "    MWETokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a sentence in English\n",
    "sentence_en = \"Hello Where are you? I'm in San Francisco. I can't come.\" \n",
    "\n",
    "# Example of a sentence in French\n",
    "sentence_fr = \"Bonjour Où es-tu? Je suis à Saint Etienne. Je ne peux pas venir.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tokenize for word tokenization\n",
    "print(\"word_tokenize: \", word_tokenize(sentence_en))\n",
    "print(\"word_tokenize: \", word_tokenize(sentence_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sent_tokenize for tokenizing the sentences\n",
    "print(\"sent_tokenize: \", sent_tokenize(sentence_en))\n",
    "print(\"sent_tokenize: \", sent_tokenize(sentence_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TreebankWordTokenizer for tokenizing the words\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(\"TreebankWordTokenizer: \", tokenizer.tokenize(sentence_en))\n",
    "print(\"TreebankWordTokenizer: \", tokenizer.tokenize(sentence_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordpunct_tokenize for tokenizing the words\n",
    "print(\"wordpunct_tokenize: \", wordpunct_tokenize(sentence_en))\n",
    "print(\"wordpunct_tokenize: \", wordpunct_tokenize(sentence_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TweetTokenizer for tokenizing the words\n",
    "Tweet_tokenizer = TweetTokenizer(match_phone_numbers=True)\n",
    "print(\"TweetTokenizer: \", Tweet_tokenizer.tokenize(sentence_en))\n",
    "print(\"TweetTokenizer: \", Tweet_tokenizer.tokenize(sentence_fr))\n",
    "another_sentence_phone = \"Call me at 123-456-7890\"\n",
    "print(\"TweetTokenizer: \", Tweet_tokenizer.tokenize(another_sentence_phone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MWEtokenizer_mwes = MWETokenizer(mwes=[('San', 'Francisco'), ('I', \"'m\"), ('Saint', \"Etienne\")], separator=\"-\")\n",
    "MWEtokenizer = MWETokenizer()\n",
    "\n",
    "print(\"MWETokenizer without MWE: \", MWEtokenizer.tokenize(word_tokenize(sentence_en)))\n",
    "print(\"MWETokenizer with MWE: \", MWEtokenizer_mwes.tokenize(word_tokenize(sentence_en)))\n",
    "\n",
    "print(\"MWETokenizer without MWE: \", MWEtokenizer.tokenize(word_tokenize(sentence_fr)))\n",
    "print(\"MWETokenizer with MWE: \", MWEtokenizer_mwes.tokenize(word_tokenize(sentence_fr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U textblob\n",
    "!python3 -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the sentence\n",
    "blob = TextBlob(sentence_en)\n",
    "print(\"TextBlob: \", blob.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the sentence in french\n",
    "blob = TextBlob(sentence_fr)\n",
    "print(\"TextBlob for French: \", blob.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input texts in english\n",
    "text1 = \"Hello! How are you?\"\n",
    "text2 = \"Let's visit www.example.com!\"\n",
    "\n",
    "# Example input texts in french\n",
    "text3 = \"Bonjour! Comment ça va?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text using spaCy's default tokenizer\n",
    "# nlp here is the spaCy model loaded in the previous step\n",
    "\n",
    "print(\"spaCy tokenizer: \", [token.text for token in nlp(text1)])\n",
    "print(\"spaCy tokenizer: \", [token.text for token in nlp(text2)])\n",
    "print(\"spaCy tokenizer: \", [token.text for token in nlp(text3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and tokenize a stream of texts\n",
    "texts = [\"Hello!\", \"How are you?\"]\n",
    "docs = nlp.pipe(texts)\n",
    "tokens_pipe = [[token.text for token in doc] for doc in docs]\n",
    "print(\"Pipe Tokenizer:\", tokens_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Tokenizer in spaCy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank Tokenizer with just the English vocab\n",
    "custom_nlp = English()\n",
    "custom_tokenizer = Tokenizer(custom_nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example custom tokenization\n",
    "doc2 = custom_tokenizer(text2)\n",
    "tokens2 = [token.text for token in doc2]\n",
    "print(\"Custom Tokenizer:\", tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization with gensim\n",
    "from gensim.utils import tokenize\n",
    "\n",
    "# English sentence\n",
    "tokens = tokenize(sentence_en)\n",
    "print(\"gensim tokenizer in English: \", list(tokens))\n",
    "\n",
    "# French sentence\n",
    "tokens = tokenize(sentence_fr)\n",
    "print(\"gensim tokenizer in French: \", list(tokens))\n",
    "\n",
    "# French sentence with accents\n",
    "tokens = tokenize(sentence_fr, deacc=True)\n",
    "print(\"gensim tokenizer in French without accents: \", list(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words\n",
    "\n",
    "Stopwords are common words (like \"and,\" \"the,\" \"in\") that carry little semantic value in text analysis. Removing them helps reduce noise and focus on the more meaningful words in a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download stopwords from nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the list of stopwords in english\n",
    "print(\"List of stopwords in English: \", stopwords.words('english'))\n",
    "print(\"Number of stopwords in English: \", len(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the list of stopwords in another language (french)\n",
    "print(\"List of stopwords in French: \", stopwords.words('french'))\n",
    "print(\"Number of stopwords in French: \", len(stopwords.words('french')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the stopwords from the sentences \n",
    "filtered_sentence_en = [word for word in word_tokenize(sentence_en) if word.lower() not in stopwords.words('english')]\n",
    "print(\"Filtered sentence in English: \", filtered_sentence_en)\n",
    "\n",
    "filtered_sentence_fr = [word for word in word_tokenize(sentence_fr) if word.lower() not in stopwords.words('french')]\n",
    "print(\"Filtered sentence in French: \", filtered_sentence_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_spacy = stop_words.STOP_WORDS\n",
    "print(\"List of stopwords in English using spaCy: \", stopwords_spacy)\n",
    "print(\"Number of stopwords in English using spaCy: \", len(stopwords_spacy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR \n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get the list of stopwords in English\n",
    "stopwords_spacy = list(nlp.Defaults.stop_words)\n",
    "print(\"List of stopwords in English: \", stopwords_spacy)\n",
    "print(\"Number of stopwords in English: \", len(stopwords_spacy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the French model\n",
    "!python -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the French model\n",
    "import fr_core_news_md\n",
    "\n",
    "# Get the list of stopwords in French\n",
    "nlp_fr = fr_core_news_md.load()\n",
    "stopwords_spacy_fr = list(nlp_fr.Defaults.stop_words)\n",
    "print(\"List of stopwords in French: \", stopwords_spacy_fr)\n",
    "print(\"Number of stopwords in French: \", len(stopwords_spacy_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the stopwords from the sentences\n",
    "filtered_sentence_spacy_en = [word for word in word_tokenize(sentence_en) if word.lower() not in stopwords_spacy]\n",
    "print(\"Filtered sentence in English: \", filtered_sentence_spacy_en)\n",
    "\n",
    "filtered_sentence_spacy_fr = [word for word in word_tokenize(sentence_fr) if word.lower() not in stopwords_spacy_fr]\n",
    "print(\"Filtered sentence in French: \", filtered_sentence_spacy_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the list of stopwords in gensim\n",
    "\n",
    "print(\"List of stopwords in gensim: \", STOPWORDS)\n",
    "print(\"Number of stopwords in gensim: \", len(STOPWORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using gensim to remove stopwords\n",
    "remove_stopwords(sentence_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    "\n",
    "Lemmatization is the process of converting a word to its base or dictionary form (lemma). For example, \"running\" becomes \"run.\" Lemmatization helps in reducing the different forms of a word to a single form, which aids in standardizing text for analysis.\n",
    "\n",
    "Stemming is the process of reducing a word to its root or base form, often by stripping away prefixes or suffixes. Unlike lemmatization, which returns the actual dictionary form of a word, stemming usually results in a word stem that may not be a valid word in the language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import different stemmers\n",
    "from nltk.stem import (\n",
    "    PorterStemmer,\n",
    "    LancasterStemmer,\n",
    "    SnowballStemmer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a list of different words\n",
    "words = [\"car\", \"cars\", \"bus\", \"buses\", \"fly\", \"flies\", \"run\", \"running\", \"city\", \"cities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the stemmers\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem the words using the stemmers\n",
    "print(\"Porter Stemmer: \", [porter.stem(word) for word in words])\n",
    "print(\"Lancaster Stemmer: \", [lancaster.stem(word) for word in words])\n",
    "print(\"Snowball Stemmer: \", [snowball.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using the nlp object created in the previous steps\n",
    "print(\"Spacy Lemmatization: \", [token.lemma_ for token in nlp(\" \".join(words))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be using the TextBlob object created in the previous steps\n",
    "blob = TextBlob(\" \".join(words))\n",
    "print(\"TextBlob Lemmatization: \", blob.words.lemmatize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    "\n",
    "Part-of-Speech (POS) Tagging is the process of assigning a part of speech (like noun, verb, adjective) to each word in a sentence. It helps in understanding the grammatical structure and the meaning of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the pos tagger from nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag the words in the sentence for the English language\n",
    "nltk.pos_tag(word_tokenize(sentence_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag the words in the sentence for the French language\n",
    "nltk.pos_tag(word_tokenize(sentence_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a ConditionalFreqDist object\n",
    "CFD = nltk.ConditionalFreqDist(nltk.pos_tag(word_tokenize(sentence_en)))\n",
    "# get all the tags and their frequency\n",
    "print(CFD.tabulate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of POS tags\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos tagging using spacy\n",
    "\n",
    "print(\"spaCy POS tagging: \", [(token.text, token.pos_) for token in nlp(sentence_en)])\n",
    "print(\"spaCy POS tagging: \", [(token.text, token.pos_) for token in nlp(sentence_fr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some examoles of POS tags in spaCy\n",
    "print(\"List of POS tags in spaCy: \", spacy.explain(\"ADJ\"))\n",
    "print(\"List of POS tags in spaCy: \", spacy.explain(\"DET\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the TextBlob object created in the previous steps\n",
    "print(\"TextBlob POS tagging: \", blob.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos tagging in french\n",
    "blob_fr = TextBlob(sentence_fr)\n",
    "print(\"TextBlob POS tagging in French: \", blob_fr.tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
